{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9001c9fd-5c54-43d0-9ed2-bc2c72cf7911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Chatbot\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Vishesh8/databricks-tests/refs/heads/main/training-images/rag.jpeg\" width=\"1368\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d027dece-7dfb-4877-9f2e-38fb89239255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbotocore 1.34.39 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.3.0 which is incompatible.\ngoogle-api-core 2.18.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0m\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU openai databricks-langchain langchain-chroma pypdf docarray gradio\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb27158-4d0f-444a-bdc1-0fae627daa9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "DATABRICKS_TOKEN = dbutils.secrets.get(scope = \"db-field-eng\", key = \"va-pat-token\")\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=DATABRICKS_TOKEN,\n",
    "  base_url=\"https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d3314cb-b753-409a-b8d2-37c24992fccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a9cf5e-5379-48dd-b7ac-4d18adef9d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n"
     ]
    }
   ],
   "source": [
    "# Set Temperature = 0 for generation model in our Q&A application for low variability and factual answers\n",
    "llm = ChatDatabricks(endpoint=\"databricks-claude-3-7-sonnet\", temperature=0)\n",
    "embedding = DatabricksEmbeddings(endpoint=\"databricks-gte-large-en\")\n",
    "\n",
    "persist_directory = './data/docs/chroma/'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "143bbd06-a881-4ca0-80f1-f4e4560aedcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0131fb51-0a50-4650-bb2e-4fc7ff648b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Probability is not a main class topic but is a prerequisite knowledge that students are expected to have. The instructor assumes familiarity with basic probability and statistics, including concepts like random variables, expectation, and variance, but will offer refreshers in discussion sections for those who need it. Thanks for asking!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "question = \"Is probability a class topic?\"\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "  llm=llm,\n",
    "  retriever=vectordb.as_retriever(),\n",
    "  return_source_documents=True,\n",
    "  chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "  )\n",
    "\n",
    "result = qa_chain.invoke(question)\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "951a117a-2eb2-46f4-ac74-7c39a4772fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Memory\n",
    "`ConversationBufferMemory` keeps chat messages in the history and passes that to the chatbot along with the question everytime. We'll also specify a `memory_key` that will line up with one of the `input_variable` in the prompt. `return_messages` is set to `True` to return chat history as list of messages as opposed to a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf04cf9-2c5f-45f1-913b-41f6b6387c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "  memory_key=\"chat_history\",\n",
    "  return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49119b51-b5d1-4e2e-85c7-ef8dc366a63c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ConversationalRetrieval Chain\n",
    "\n",
    "We'll introduce a new type of chain called `ConversationalRetrievalChain` that is similar to QARetrieval chain but also takes in a `LangChain \n",
    "memory` argument that allows us to take the chat history in the context for answering follow-up questions. Note that different `retrieval` strategies like `self-query`, `compression`, etc. as well as different `generation` approaches with various `chain_type` e.g., stuff, refine, etc. are supported by this chain\n",
    "\n",
    "`ConversationalRetrievalChain` not only adds `memory` on `QARetrieverChain` but it also adds a step that takes the history, along with the new question, and condenses it to a standalone question to do the `retrieval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3760baab-aa56-4da2-809e-fcb45f467e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8ccb64-f61b-47ba-9769-f9e49a0cdb55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "  llm=llm,\n",
    "  retriever=vectordb.as_retriever(search_type=\"mmr\"),\n",
    "  memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a7d1b7-28d3-4803-83c5-e446a4be09dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Yes, probability is a prerequisite for the class rather than a main topic. The instructor mentions that they assume familiarity with basic probability and statistics, including concepts like random variables, expectation, and variance. For students who need a refresher on probability, they mention that some discussion sections will go over these prerequisites. The class itself (CS229, which appears to be a machine learning course) builds on this probabilistic foundation rather than teaching probability as a primary topic.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-b0f246a0b9d64ad792f27ad90d94245e\"",
      "text/plain": [
       "Trace(request_id=tr-b0f246a0b9d64ad792f27ad90d94245e)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "\n",
    "result = qa.invoke(question)\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0d7ca3-095f-4842-b145-4a5d440d9cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Based on the context provided, probability and statistics are prerequisites for the CS229 machine learning course because:\\n\\n1. The course builds on concepts like random variables, expectation, and variance\\n2. These statistical concepts are fundamental to understanding machine learning algorithms and models\\n3. The instructor mentions that an undergraduate statistics class like Stat 116 at Stanford would be sufficient preparation\\n4. While the course offers some refresher material in discussion sections for students who haven't used statistics recently, the core lectures assume this background knowledge\\n\\nThe course appears to use these statistical foundations when developing concepts like the probabilistic interpretation of linear regression, which is mentioned as leading into logistic regression and other classification algorithms.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-6d5c4e8a149446f1b0e1bdf61b71b4ab\"",
      "text/plain": [
       "Trace(request_id=tr-6d5c4e8a149446f1b0e1bdf61b71b4ab)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"why are those prerequesites needed?\"\n",
    "\n",
    "result = qa.invoke(question)\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94cbe514-621b-4e58-93da-eb72cbbe82cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the above trace we see that before even retriever step, we also have a `chat_history` that is coming from the memory (`memory_key=\"chat_history\"`). First LLM call rephrases the follow up question based on the chat history which also has essence of the first question asked.  This standalone question then goes through the usual RAG process to generate response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff9938a8-61bc-4ac7-aaff-5bb8e4a694ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chatbot App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d484e2-3a1f-492d-875d-8ddc1cf112f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.langchain.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff77743-72c6-4c39-81ee-064d79ed0a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30669129-8f78-4644-aa8f-28648901f524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_db(file, chain_type, k):\n",
    "\n",
    "  \"\"\"This function loads a pdf file into an in-memory vector store and returns a ConversationalRetrievalChain with teh defined chain type and number of chunks to retrieve.\n",
    "  file: PDF file path to load on which to do Q&A\n",
    "  chain_type: type of chain to use for generation\n",
    "  k: number of chunks to retrieve\"\"\"\n",
    "\n",
    "  # load documents\n",
    "  loader = PyPDFLoader(file)\n",
    "  documents = loader.load()\n",
    "  \n",
    "  # split documents\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  \n",
    "  # define embedding\n",
    "  embeddings = DatabricksEmbeddings(endpoint=\"databricks-gte-large-en\")\n",
    "  \n",
    "  # create vector database from data\n",
    "  db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "  \n",
    "  # define retriever\n",
    "  retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": k})\n",
    "  \n",
    "  # create a chatbot chain. Memory is managed externally.\n",
    "  qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatDatabricks(endpoint=\"databricks-claude-3-7-sonnet\", temperature=0), \n",
    "    chain_type=chain_type, \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    "    return_generated_question=True,\n",
    "  )\n",
    "  \n",
    "  return qa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40acb178-e6d9-4a70-8592-d5212b08b0e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that in the above code, we are not passing the memory. We'll manage it externally for the convenience of GUI implementation. This means chat history will have to be managed outside the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5faf6a8-92f4-49f8-a3eb-c4b576f63f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self):\n",
    "        self.loaded_file = \"./data/docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
    "        self.chat_history = []\n",
    "        self.db_query = \"\"\n",
    "        self.db_response = []\n",
    "        self.qa = load_db(self.loaded_file, \"stuff\", 4)\n",
    "\n",
    "    def load_file(self, file_obj):\n",
    "        if file_obj is None:\n",
    "            return f\"Loaded File: {self.loaded_file}\"\n",
    "        else:\n",
    "            with open(\"temp.pdf\", \"wb\") as f:\n",
    "                f.write(file_obj.read())\n",
    "            self.loaded_file = file_obj.name\n",
    "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
    "            self.clear_history()\n",
    "            return f\"Loaded File: {self.loaded_file}\"\n",
    "\n",
    "    def process_query(self, query):\n",
    "        if not query:\n",
    "            return self.chat_history\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        # Append new exchange as a tuple (user, bot)\n",
    "        self.chat_history.append((query, result[\"answer\"]))\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        return self.chat_history\n",
    "\n",
    "    def get_db_query(self):\n",
    "        if not self.db_query:\n",
    "            return \"No DB accesses so far.\"\n",
    "        return f\"**DB Query:** {self.db_query}\"\n",
    "\n",
    "    def get_db_sources(self):\n",
    "        if not self.db_response:\n",
    "            return \"No DB response yet.\"\n",
    "        sources_md = \"**Result of DB lookup:**\\n\\n\"\n",
    "        for doc in self.db_response:\n",
    "            sources_md += f\"- {doc}\\n\"\n",
    "        return sources_md\n",
    "\n",
    "    def get_chat_history_text(self):\n",
    "        if not self.chat_history:\n",
    "            return \"No chat history yet.\"\n",
    "        history_md = \"\"\n",
    "        for query, answer in self.chat_history:\n",
    "            history_md += f\"**User:** {query}\\n\\n**ChatBot:** {answer}\\n\\n\"\n",
    "        return history_md\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.chat_history = []\n",
    "\n",
    "# Global session instance\n",
    "session = ChatSession()\n",
    "\n",
    "# Wrapper functions for Gradio callbacks\n",
    "def gr_load_file(file_obj):\n",
    "    return session.load_file(file_obj)\n",
    "\n",
    "def gr_process_query(query):\n",
    "    conv = session.process_query(query)\n",
    "    # The gr.Chatbot component accepts a list of (user, bot) pairs.\n",
    "    return conv\n",
    "\n",
    "def gr_get_db_query():\n",
    "    return session.get_db_query()\n",
    "\n",
    "def gr_get_db_sources():\n",
    "    return session.get_db_sources()\n",
    "\n",
    "def gr_get_chat_history():\n",
    "    return session.get_chat_history_text()\n",
    "\n",
    "def gr_clear_history():\n",
    "    session.clear_history()\n",
    "    return \"Chat history cleared.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7732a553-56ec-42b2-95fd-7d4a2373f4ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Custom CSS for enhanced styling\n",
    "custom_css = \"\"\"\n",
    "body {\n",
    "    background-color: #f7f7f7;\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "}\n",
    "h1 {\n",
    "    color: #333;\n",
    "    text-align: center;\n",
    "}\n",
    ".gradio-container {\n",
    "    border-radius: 12px;\n",
    "    box-shadow: 0 4px 12px rgba(0,0,0,0.1);\n",
    "}\n",
    ".gr-button {\n",
    "    background-color: #4CAF50 !important;\n",
    "    color: white !important;\n",
    "    border-radius: 8px;\n",
    "}\n",
    "#chatbot {\n",
    "    background-color: #ffffff;\n",
    "    border-radius: 8px;\n",
    "    padding: 10px;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a9f24b-9b55-46b1-87c9-a37f96f75eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with gr.Blocks(css=custom_css, title=\"ChatWithYourData Bot\") as demo:\n",
    "    gr.Markdown(\"<h1>ChatWithYourData Bot</h1>\")\n",
    "    with gr.Tabs():\n",
    "        # --- Conversation Tab ---\n",
    "        with gr.Tab(\"Conversation\"):\n",
    "            with gr.Row():\n",
    "                chatbot = gr.Chatbot(label=\"Conversation\", elem_id=\"chatbot\")\n",
    "            with gr.Row():\n",
    "                query_input = gr.Textbox(placeholder=\"Enter your question here...\", label=\"Your Query\")\n",
    "                submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            submit_btn.click(fn=gr_process_query, inputs=query_input, outputs=chatbot)\n",
    "            query_input.submit(fn=gr_process_query, inputs=query_input, outputs=chatbot)\n",
    "        \n",
    "        # --- Database Tab ---\n",
    "        with gr.Tab(\"Database\"):\n",
    "            with gr.Column():\n",
    "                db_query_box = gr.Markdown(label=\"DB Query\")\n",
    "                db_sources_box = gr.Markdown(label=\"DB Sources\")\n",
    "                refresh_db_btn = gr.Button(\"Refresh Database Info\", variant=\"primary\")\n",
    "            refresh_db_btn.click(fn=lambda: (gr_get_db_query(), gr_get_db_sources()),\n",
    "                                 inputs=[], outputs=[db_query_box, db_sources_box])\n",
    "        \n",
    "        # --- Chat History Tab ---\n",
    "        with gr.Tab(\"Chat History\"):\n",
    "            with gr.Column():\n",
    "                chat_history_box = gr.Markdown(label=\"Chat History\")\n",
    "                refresh_history_btn = gr.Button(\"Refresh Chat History\", variant=\"primary\")\n",
    "            refresh_history_btn.click(fn=gr_get_chat_history, inputs=[], outputs=chat_history_box)\n",
    "        \n",
    "        # --- Configure Tab ---\n",
    "        with gr.Tab(\"Configure\"):\n",
    "            with gr.Row():\n",
    "                file_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "                load_btn = gr.Button(\"Load DB\", variant=\"primary\")\n",
    "            load_status = gr.Markdown(label=\"Load Status\")\n",
    "            load_btn.click(fn=gr_load_file, inputs=file_input, outputs=load_status)\n",
    "            with gr.Row():\n",
    "                clear_history_btn = gr.Button(\"Clear History\", variant=\"secondary\")\n",
    "                clear_status = gr.Markdown(label=\"Clear History Status\")\n",
    "            clear_history_btn.click(fn=gr_clear_history, inputs=[], outputs=clear_status)\n",
    "            gr.Markdown(\"Clears chat history. Use to start a new topic.\")\n",
    "            gr.Image(\"./data/img/convchain.jpg\", label=\"Conversation Chain\", show_label=True, elem_id=\"convchain_img\", height=300)\n",
    "\n",
    "demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 990384398398040,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "06-chatbot",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
