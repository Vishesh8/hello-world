{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9001c9fd-5c54-43d0-9ed2-bc2c72cf7911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question Answering\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Vishesh8/databricks-tests/refs/heads/main/training-images/question-answering.png\" width=\"768\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d027dece-7dfb-4877-9f2e-38fb89239255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU openai databricks-langchain langchain-chroma transformers langsmith\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb27158-4d0f-444a-bdc1-0fae627daa9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "DATABRICKS_TOKEN = dbutils.secrets.get(scope = \"db-field-eng\", key = \"va-pat-token\")\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=DATABRICKS_TOKEN,\n",
    "  base_url=\"https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d3314cb-b753-409a-b8d2-37c24992fccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a9cf5e-5379-48dd-b7ac-4d18adef9d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n"
     ]
    }
   ],
   "source": [
    "# Set Temperature = 0 for generation model in our Q&A application for low variability and factual answers\n",
    "llm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", temperature=0)\n",
    "embedding = DatabricksEmbeddings(endpoint=\"databricks-gte-large-en\")\n",
    "\n",
    "persist_directory = './data/docs/chroma/'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "537a48f6-0a66-44ba-b7a3-f8d3312ec792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"okay?  \\nSo as an overview of what we're going to do in this class, this class is sort of organized \\ninto four major sections. We're gonna talk about four major topics in this class, the first \\nof which is supervised learning. So let me give you an example of that.  \\nSo suppose you collect a data set of housing prices. And one of the TAs, Dan Ramage, \\nactually collected a data set for me last week to use in the example later. But suppose that \\nyou go to collect statistics about how much houses co\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs = vectordb.similarity_search(question, k=3)\n",
    "\n",
    "docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "183c42fa-84cc-4214-9ade-9b52c2bf1215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## RetrievalQA Chain\n",
    "\n",
    "General flow for Q&A chain:\n",
    "- question comes in\n",
    "- lookup relevant documents\n",
    "- pass retrieved chunks along with a `system prompt` and `human question` to the LLM to generate a response\n",
    "\n",
    "`chain_type` by default is `stuff`, e.g., we just pass all the chunks into the same context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90dd6881-fe30-4a03-b702-110875e19381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8cc813-36f7-4d18-9f96-b9a3b4d7b94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new StuffDocumentsChain chain...\u001B[0m\n\n\n\u001B[1m> Entering new LLMChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mSystem: Use the following pieces of context to answer the user's question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\nokay?  \nSo as an overview of what we're going to do in this class, this class is sort of organized \ninto four major sections. We're gonna talk about four major topics in this class, the first \nof which is supervised learning. So let me give you an example of that.  \nSo suppose you collect a data set of housing prices. And one of the TAs, Dan Ramage, \nactually collected a data set for me last week to use in the example later. But suppose that \nyou go to collect statistics about how much houses cost in a certain geographic area. And \nDan, the TA, collected data from housing prices in Portland, Oregon. So what you can do \nis let's say plot the square footage of the house against the list price of the house, right, so \nyou collect data on a bunch of houses. And let's say you get a data set like this with \nhouses of different sizes that are listed for different amounts of money.  \nNow, let's say that I'm trying to sell a house in the same area as Portland, Oregon as \nwhere the data comes from. Let's say I have a house that's this size in square footage, and \nI want an algorithm to tell me about how much should I expect my house to sell for. So \nthere are lots of ways to do this, and some of you may have seen elements of what I'm \nabout to say before.  \nSo one thing you could do is look at this data and maybe put a straight line to it. And then \nif this is my house, you may then look at the straight line and predict that my house is\n\nSo all right, online resources. The class has a home page, so it's in on the handouts. I \nwon't write on the chalkboard — http:// cs229.stanford.edu. And so when there are \nhomework assignments or things like that, we usually won't sort of — in the mission of \nsaving trees, we will usually not give out many handouts in class. So homework \nassignments, homework solutions will be posted online at the course home page.  \nAs far as this class, I've also written, and I guess I've also revised every year a set of \nfairly detailed lecture notes that cover the technical content of this class. And so if you \nvisit the course homepage, you'll also find the detailed lecture notes that go over in detail \nall the math and equations and so on that I'll be doing in class.  \nThere's also a newsgroup, su.class.cs229, also written on the handout. This is a \nnewsgroup that's sort of a forum for people in the class to get to know each other and \nhave whatever discussions you want to have amongst yourselves. So the class newsgroup \nwill not be monitored by the TAs and me. But this is a place for you to form study groups \nor find project partners or discuss homework problems and so on, and it's not monitored \nby the TAs and me. So feel free to talk trash about this class there.  \nIf you want to contact the teaching staff, please use the email address written down here, \ncs229-qa@cs.stanford.edu. This goes to an account that's read by all the TAs and me. So\n\nof this class will not be very programming intensive, although we will do some \nprogramming, mostly in either MATLAB or Octave. I'll say a bit more about that later.  \nI also assume familiarity with basic probability and statistics. So most undergraduate \nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \nassume all of you know what random variables are, that all of you know what expectation \nis, what a variance or a random variable is. And in case of some of you, it's been a while \nsince you've seen some of this material. At some of the discussion sections, we'll actually \ngo over some of the prerequisites, sort of as a refresher course under prerequisite class. \nI'll say a bit more about that later as well.  \nLastly, I also assume familiarity with basic linear algebra. And again, most undergraduate \nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \ngonna assume that all of you know what matrixes and vectors are, that you know how to \nmultiply matrices and vectors and multiply matrix and matrices, that you know what a \nmatrix inverse is. If you know what an eigenvector of a matrix is, that'd be even better. \nBut if you don't quite know or if you're not quite sure, that's fine, too. We'll go over it in \nthe review sections.\n\nmany biologers are there here? Wow, just a few, not many. I'm surprised. Anyone from \nstatistics? Okay, a few. So where are the rest of you from?  \nStudent : iCME.  \nInstructor (Andrew Ng) : Say again?  \nStudent : iCME.  \nInstructor (Andrew Ng) : iCME. Cool.  \nStudent : [Inaudible].  \nInstructor (Andrew Ng) : Civi and what else?  \nStudent : [Inaudible]  \nInstructor (Andrew Ng) : Synthesis, [inaudible] systems. Yeah, cool.  \nStudent : Chemi.  \nInstructor (Andrew Ng) : Chemi. Cool.  \nStudent : [Inaudible].  \nInstructor (Andrew Ng) : Aero/astro. Yes, right. Yeah, okay, cool. Anyone else?  \nStudent : [Inaudible].  \nInstructor (Andrew Ng) : Pardon? MSNE. All right. Cool. Yeah.  \nStudent : [Inaudible].  \nInstructor (Andrew Ng) : Pardon?  \nStudent : [Inaudible].  \nInstructor (Andrew Ng) : Endo —  \nStudent : [Inaudible].  \nInstructor (Andrew Ng) : Oh, I see, industry. Okay. Cool. Great, great. So as you can \ntell from a cross-section of this class, I think we're a very diverse audience in this room, \nand that's one of the things that makes this class fun to teach and fun to be in, I think.\nHuman: What are major topics for this class?\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-a009eb36-d7e5-4f78-873b-406a815d5179/lib/python3.11/site-packages/databricks/sdk/errors/base.py:87: UserWarning: The 'retry_after_secs' parameter of DatabricksError is deprecated and will be removed in a future version.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\u001B[1m> Finished chain.\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'query': 'What are major topics for this class?',\n",
       " 'result': 'The class is organized into four major sections, and the first major topic is supervised learning. The other three topics are not specified in the provided context.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "  llm=llm,\n",
    "  retriever=vectordb.as_retriever(search_type=\"mmr\"),   # MMR added to remove duplicate chunks\n",
    "  chain_type_kwargs={\"verbose\":True}\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke(question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9504085c-f07a-4a98-b1ab-ba6bf2801b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c542a35c-13dd-4b59-83d1-bcbf3b110708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f046faed-da5c-489c-9de3-b37c9d922879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "  llm=llm,\n",
    "  retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3}),\n",
    "  return_source_documents=True,                     # Inspect Retrieved docs\n",
    "  chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT, \"verbose\":True}     # Use this prompt in the chain instead of default that doesn't contain instructions around response length and thanking at the end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b61de0-cfc3-4964-9357-22f38bfe5070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new StuffDocumentsChain chain...\u001B[0m\n\n\n\u001B[1m> Entering new LLMChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \nstatistics for a while or maybe algebra, we'll go over those in the discussion sections as a \nrefresher for those of you that want one.  \nLater in this quarter, we'll also use the discussion sections to go over extensions for the \nmaterial that I'm teaching in the main lectures. So machine learning is a huge field, and \nthere are a few extensions that we really want to teach but didn't have time in the main \nlectures for.\n\nof this class will not be very programming intensive, although we will do some \nprogramming, mostly in either MATLAB or Octave. I'll say a bit more about that later.  \nI also assume familiarity with basic probability and statistics. So most undergraduate \nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \nassume all of you know what random variables are, that all of you know what expectation \nis, what a variance or a random variable is. And in case of some of you, it's been a while \nsince you've seen some of this material. At some of the discussion sections, we'll actually \ngo over some of the prerequisites, sort of as a refresher course under prerequisite class. \nI'll say a bit more about that later as well.  \nLastly, I also assume familiarity with basic linear algebra. And again, most undergraduate \nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \ngonna assume that all of you know what matrixes and vectors are, that you know how to \nmultiply matrices and vectors and multiply matrix and matrices, that you know what a \nmatrix inverse is. If you know what an eigenvector of a matrix is, that'd be even better. \nBut if you don't quite know or if you're not quite sure, that's fine, too. We'll go over it in \nthe review sections.\n\ncome back to this again. Any questions about this? Actually, let me clean up another \ncouple of boards and then I’ll see what questions you have.  \nOkay. Any questions? Yeah?  \nStudent:You are, I think here you try to measure the likelihood of your nice of theta by a \nfraction of error, but I think it’s that you measure because it depends on the family of \ntheta too, for example. If you have a lot of parameters [inaudible] or fitting in?\nQuestion: Is probability a class topic?\nHelpful Answer:\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-a009eb36-d7e5-4f78-873b-406a815d5179/lib/python3.11/site-packages/databricks/sdk/errors/base.py:87: UserWarning: The 'retry_after_secs' parameter of DatabricksError is deprecated and will be removed in a future version.\n  warnings.warn(\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a009eb36-d7e5-4f78-873b-406a815d5179/lib/python3.11/site-packages/databricks/sdk/errors/base.py:87: UserWarning: The 'retry_after_secs' parameter of DatabricksError is deprecated and will be removed in a future version.\n  warnings.warn(\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a009eb36-d7e5-4f78-873b-406a815d5179/lib/python3.11/site-packages/databricks/sdk/errors/base.py:87: UserWarning: The 'retry_after_secs' parameter of DatabricksError is deprecated and will be removed in a future version.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\u001B[1m> Finished chain.\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The class assumes familiarity with basic probability and statistics, and will review some of these concepts in discussion sections as a refresher. Probability is a prerequisite for the class, but it will not be a primary topic of focus. thanks for asking!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "\n",
    "result = qa_chain.invoke(question)\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f0fdd31-515f-4d1c-8509-ed8b967e7012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(id='9b309f2a-af07-49f1-b087-82a1140a7e4e', metadata={'author': '', 'creationdate': '2008-07-11T11:25:23-07:00', 'creator': 'PScript5.dll Version 5.2.2', 'moddate': '2008-07-11T11:25:23-07:00', 'page': 8, 'page_label': '9', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'source': './data/docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'title': '', 'total_pages': 22}, page_content=\"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the discussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectures. So machine learning is a huge field, and \\nthere are a few extensions that we really want to teach but didn't have time in the main \\nlectures for.\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70d498a2-df85-425f-83dd-a1367f409f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### RetrievalQA Chain Types\n",
    "Different chain types can also be used for applications other than question-answering. For example, one common use case for `map_reduce` chain is summarization task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd518ed-4834-44f0-a08e-8bf982aa8a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. stuff\n",
    "Simply stuffs all data into the prompt as context to pass to the language model. This is the most common chain type method used and the default one if we don't explicitly pass this parameter\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Vishesh8/databricks-tests/refs/heads/main/training-images/chain-type-stuff.png\" width=\"768\">\n",
    "\n",
    "**Pros:** makes a single call to the LLM and LLM has access to all the data at once. It's quite simple to understand and cheap\n",
    "\n",
    "**Cons:** LLMs have a context length, and for large or many documents this might result in a prompt larger than the context length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dc35c0e-8ecf-4373-8bf7-965acfd1d209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. map_reduce\n",
    "In this technique, each of the individual retrieved doc is sent alongwith a question to a language model to get the answer and then uses another LLM call to summarize all the individual responses into a final answer\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Vishesh8/databricks-tests/refs/heads/main/training-images/chain-type-map-reduce.png\" width=\"768\">\n",
    "\n",
    "**Pros:** powerful as it can operate over arbitrarily any number of documents and we can also do individual questions in parallel\n",
    "\n",
    "**Cons:** it makes a lot more LLM calls and it also treats all documents as independent which may not be the most desired thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03885d90-43e7-4ced-9b7d-4c02c5109971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note that prompt override is not applicable for Map Reduce\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "  llm=llm,\n",
    "  retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10}),   # Can include more chunks with map reduce as it operates on individual chunks separately so context window limitation doesn't hit\n",
    "  chain_type=\"map_reduce\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9696f59-ac9e-4d08-8607-945ea92a193b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\nUnexpected internal error when monkey patching `PreTrainedModel.from_pretrained`: \nPreTrainedModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n\nUnexpected internal error when monkey patching `Trainer.train`: \nTrainer requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"It appears that probability is likely a relevant topic in the class, although the text does not always explicitly mention it. In some cases, the text mentions statistics, machine learning, or other related fields that often involve probability, and in one case, it explicitly states that the instructor assumes familiarity with basic probability and statistics. However, without more context, it's difficult to say for certain whether probability is a specific class topic. Based on the available information, it seems probable that probability is a class topic, but it's not a definitive answer.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain_mr.invoke(question)\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51883a1f-e536-42d5-b4f4-3aca382ba199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To understand what's going on inside these chains, we can use `mlflow tracing` on Databricks.\n",
    "If you want to use `LangSmith Platform`. Pre-requisites: \n",
    "- Go to [LangSmith](https://www.langchain.com/langsmith) and sign up\n",
    "- Create an API key from your account's settings\n",
    "- Uncomment below code: \\\n",
    "<code>os.environ[\"LANGSMITH_TRACING\"] = \"true\"</code> \\\n",
    "<code>os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"</code> \\\n",
    "<code>os.environ[\"LANGSMITH_API_KEY\"] = \"<lc_api_key>\"</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b0d9aae-a1ae-4ad3-94be-b20d7d307d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Yes, probability is a relevant topic, as the instructor assumes familiarity with basic probability and statistics, and mentions that it may be covered as a refresher topic in discussion sections.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-010819ca8acb4dd18f30b0ea27581ef3\"",
      "text/plain": [
       "Trace(request_id=tr-010819ca8acb4dd18f30b0ea27581ef3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "  llm=llm,\n",
    "  retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10}),    # Can include more chunks with map reduce as it operates on individual chunks separately so context window limitation doesn't hit\n",
    "  chain_type=\"map_reduce\",\n",
    ")\n",
    "\n",
    "result = qa_chain_mr.invoke(question)\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dd5f18a-9d2a-4095-8ee8-51b7c194b546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From the above trace, we can see there was one LLM call for each chunk and then one final call that had response from all the chunks as context to generate the final response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff5e7d6b-d48c-415c-8772-943384408192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. refine\n",
    "This is also used for many documents like map reduce but it builds upon the answer from previous document iteratively\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Vishesh8/databricks-tests/refs/heads/main/training-images/chain-type-refine.png\" width=\"768\">\n",
    "\n",
    "**Pros:** great for combining information and building up answer over time where documents might be dependent\n",
    "\n",
    "**Cons:** generally leads to longer answers. Number of calls are same as map_reduce, but it's not as fast as the LLM calls aren't independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0948c730-bcd3-4629-b185-fd87654d88d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Yes, probability is likely a class topic. The context suggests that the class is focused on machine learning, and the conversation between the instructor (Andrew Ng) and the student involves discussing mathematical concepts and algorithms, such as least squares regression. This implies that the class covers theoretical foundations of machine learning, which often involve probability theory. Therefore, it is reasonable to assume that probability will be covered in the class.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-6203a6dbbcd2498890f0c7b4ae81969b\"",
      "text/plain": [
       "Trace(request_id=tr-6203a6dbbcd2498890f0c7b4ae81969b)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa_chain_ref = RetrievalQA.from_chain_type(\n",
    "  llm=llm,\n",
    "  retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10}),\n",
    "  chain_type=\"refine\",\n",
    ")\n",
    "\n",
    "result = qa_chain_ref.invoke(question)\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5660ddf0-12b7-4650-b39e-b657a0947182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The above trace shows that there were sequential LLM call for each chunk. Each subsequent call to the language model asks for an improved response based on the previous response and current chunk as additional context. The result is also better than the `map reduce` chain because using refined chain allows to combine information sequentially and encourages more copy over of information than `map reduce`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef5829e6-e8ff-4719-a420-344381987eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4. map_rerank\n",
    "Does a single call to the LLM for each document and also asks it to return a score. Then we select the highest score. This relies on the language model to know what the score should be so we often have to tell it for higher relevance, the score is higher and refine instructions around scoring\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Vishesh8/databricks-tests/refs/heads/main/training-images/chain-type-map-rerank.png\" width=\"768\">\n",
    "\n",
    "**Pros:** all the LLM calls are independent so it can be batched and is faster\n",
    "\n",
    "**Cons:** since we are making a bunch of LLM calls, it'll be expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d26028-4dac-4b0f-9eca-b0b7d420868d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### RetrievalQA Limitations\n",
    "\n",
    "In a chat application, we get a conversational experience as we can also ask for followup questions. But RetrievalQA chain fails to preserve conversational history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827e942c-7ac0-489a-8624-2fbba7e4c826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.langchain.autolog(disable=True)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "  llm=llm,\n",
    "  retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce5a3127-cf6b-4bbf-9a2d-22acb535c394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"The class assumes familiarity with basic probability, but it doesn't appear to be a main topic of the class. Instead, the instructor mentions that they will review some probability concepts, such as random variables, expectation, and variance, in the discussion sections as a refresher for those who need it.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "\n",
    "result = qa_chain.invoke(question)\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f408878e-c06c-4a43-b993-fec79af9adcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The prerequisites are needed because the class will be covering topics that build upon those foundational concepts. \\n\\n* Big O notation, data structures, and programming skills are likely needed for implementing and analyzing algorithms.\\n* Probability and statistics are needed for understanding and working with random variables, expectations, and variances, which are probably crucial concepts in the class.\\n* Linear algebra is needed for working with matrices, vectors, and possibly eigenvectors, which are likely used in the class for solving problems or modeling systems.\\n\\nThe instructor assumes that students have a solid grasp of these concepts so that they can focus on the more advanced topics that the class will be covering, without having to spend too much time reviewing the basics.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"why are those prerequesites needed?\"\n",
    "\n",
    "result = qa_chain.invoke(question)\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74d4949-675e-4fe6-a2d2-de66abfd7ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note how the above answer doesn't relate to our first question around probability at all. The QA chain doesn't have any concept of state so it doesn't remember previous questions and answers. We can fix this with `LangChain Memory`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 990384398398040,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "05-question-answering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
